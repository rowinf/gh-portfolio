---
date: "2023-11-12T21:30:00Z"
draft: false
title: "Convert a HTML table to CSV via command line"
---

Turning a html table into a csv is an interesting problem because html parsing and csv formatting can both be very challenging.

The command line is a good approach because it is flexible enough to handle many different cases. The tools I ended up using seem powerful, so attempting this will level up your command line skills. This solution is highly flexible and powerful, but we pay the price for that power and flexibility by having to learn something.

Since we're going with a pure command line, we start with either `$ curl https://en.wikipedia.org/wiki/List_of_cities_by_sunshine_duration` or `$ cat file`.

[pup](https://github.com/ericchiang/pup) seemed like a part of the solution. It offers general purpose html parsing/selecting. The command I used was 
```$ pup '.wikitable:first-of-type td:not(:nth-of-type(16)) text{}'``` 

to get the text from each table cell, skipping the final item in this case, which is a footnote and not important to us.

this grep command filters out the new lines that pup leaves in after stripping out the tags with the `text{}` function.
```grep -v '^$'```

You can see this [more detailed explanation](https://unix.stackexchange.com/questions/589798/html-parsing-with-pup) for using pup

`awk` is a command well suited to finishing the next task. What's needed is a way to operate on each group of 15 lines together. Join each group of 15 lines with commas and a new line at the end to bring back the table cells together to form a CSV.

putting it all together you get:

```
pup '.wikitable:first-of-type td:not(:nth-of-type(16)) text{}' | 
grep -v '^$' |  
awk '{printf "%s%s", (NR%15 == 1 ? "" : ", "), $0} NR % 15 == 0 {print ""}'
```
