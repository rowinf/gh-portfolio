---
date: "2023-11-12T21:30:00Z"
draft: true
title: "AWK: convert a HTML table to CSV"
---

Turning a html table into a csv is an interesting problem because html parsing and csv formatting can both be very challenging.

The command line is a good approach because it is flexible enough to handle many different cases. The tools I ended up using seem powerful, so attempting this will level up your command line skills. This solution is highly flexible and powerful, but we pay the price for that power and flexibility by having to learn something.

Since we're going with a pure command line, we start with either `$ curl https://en.wikipedia.org/wiki/List_of_cities_by_sunshine_duration` or `$ cat file`.

[pup](https://github.com/ericchiang/pup) seemed like a part of the solution. It offers general purpose html parsing/selecting. The command I used was 
```$ pup '.wikitable:first-of-type td:not(:nth-of-type(16)) text{}'``` 

to get the text from each table cell. 

this grep command filters out the new lines that pup leaves in after stripping out the tags with the `text{}` function
```grep -v '^$'```

You can see this [more detailed explanation](https://unix.stackexchange.com/questions/589798/html-parsing-with-pup) for using pup

`awk` is the next tool I used after unsuccessfully trying xargs, sed, and paste. What eventually worked was this: `$ awk '{printf "%s%s", (NR%3 == 1 ? "" : ", "), $0} NR % 3 == 0 {print ""}'`

putting it all together you get:

```
pup '.wikitable:first-of-type td:not(:nth-of-type(16)) text{}' | 
grep -v '^$' |  
awk '{printf "%s%s", (NR%15 == 1 ? "" : ", "), $0} NR % 15 == 0 {print ""}'
```
